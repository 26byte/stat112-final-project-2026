{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Import necessary libraries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2026-01-16T16:47:37.907431900Z",
     "start_time": "2026-01-16T16:47:37.884571100Z"
    }
   },
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random as rand\n",
    "import string\n",
    "from difflib import get_close_matches\n",
    "#!pip install openpyxl # because the docker images on our cocalc servers didn't have it installed for some weird, unknown reason."
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Define necessary custom functions that will be useful when cleaning the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2026-01-16T16:51:00.985747600Z",
     "start_time": "2026-01-16T16:51:00.935543300Z"
    }
   },
   "source": [
    "# Create a function to find outliers using IQR\n",
    "def find_outliers_IQR(df):\n",
    "    # this function takes a df or only a column and calculates outliers for each value based on IQR\n",
    "\n",
    "    q1 = df.quantile(0.25)  # variable for the first quartile\n",
    "\n",
    "    q3 = df.quantile(0.75)  # variable for the third quartile\n",
    "\n",
    "    IQR = q3 - q1  # calculate the IQR\n",
    "\n",
    "    outliers = df[\n",
    "        ((df < (q1 - 1.5 * IQR)) | (df > (q3 + 1.5 * IQR)))\n",
    "    ]  # calculate outliers for every value and put them in a series\n",
    "\n",
    "    return outliers\n",
    "\n",
    "\n",
    "# Range function\n",
    "def range_column(df):\n",
    "    max = df.max()\n",
    "    min = df.min()\n",
    "    range = max - min\n",
    "    return range\n",
    "\n",
    "\n",
    "# (NA/Null) Value Percentage Calculator\n",
    "def naValues(df):\n",
    "\n",
    "    for column in df:\n",
    "\n",
    "        total_values = len(df.index)  # number of all values in a column\n",
    "\n",
    "        total_garb = (\n",
    "            df[column].isna().sum()\n",
    "        )  # number off all NA/Null values (apparently null and na are same in pandas, df.isnull() == df.isna())\n",
    "\n",
    "        garb_perc = (total_garb * 100) // total_values\n",
    "\n",
    "        print(column, \" has total of \", total_garb, \" NA/Null values\")\n",
    "        print(\"NA/Null percentage of \", column, \" is \", garb_perc, \"% \\n\")\n",
    "\n",
    "\n",
    "# A custom function to replace special characters with corresponding letters\n",
    "def replaceSpecialChars(text):\n",
    "    # Define character replacements\n",
    "    char_replacements = {\n",
    "        \"@\": \"a\",\n",
    "        \"3\": \"e\",\n",
    "        \"1\": \"i\",\n",
    "        \"0\": \"o\",\n",
    "        \"!\": \"i\",\n",
    "        \"#\": \"h\",\n",
    "        \"$\": \"s\",\n",
    "        \"5\": \"s\",\n",
    "        \"7\": \"t\",\n",
    "        \"9\": \"g\",\n",
    "    }\n",
    "    # If the input is not a string, return it as-is\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # remove empty spaces\n",
    "    text = text.strip()\n",
    "    # Replace each special character in the text\n",
    "    for char, replacement in char_replacements.items():\n",
    "        text = text.replace(char, replacement)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# A function that manipulates strings in columns.\n",
    "def str_basicclean(\n",
    "    df, column=\"\", style=\"cap\", sp_char=string.whitespace, sp_replace=False\n",
    "):  # capitalizes strings and strips whitespaces by default, it can strip anything if provided as argument.\n",
    "    try:\n",
    "        if df[column].dtype == \"object\":\n",
    "\n",
    "            if type(sp_char) == str:\n",
    "                try:\n",
    "                    df[column] = df[column].str.strip()\n",
    "                    df[column] = df[column].str.strip(sp_char)\n",
    "                except:\n",
    "                    print(\"ERROR: Strip function did not work properly\")\n",
    "            else:\n",
    "                df[column] = df[column].str.strip()\n",
    "\n",
    "            if style == \"up\":\n",
    "                df[column] = df[column].str.upper()\n",
    "            elif style == \"low\":\n",
    "                df[column] = df[column].str.lower()\n",
    "            elif style == \"cap\":\n",
    "                df[column] = df[column].str.capitalize()\n",
    "\n",
    "            if sp_replace == True:\n",
    "                df[column] = df[column].apply(replaceSpecialChars)\n",
    "    except:\n",
    "        print(f\"ERROR: Data Type of the {column} is not object.\")\n",
    "        return 0\n",
    "    return df[column]\n",
    "\n",
    "\n",
    "# A function to list all the unique values\n",
    "def printUniqueValues(df, columnKey, sort_function=None):\n",
    "    \"\"\"\n",
    "    Print unique values of a column from a df, with optional sorting.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.df): The df containing the column.\n",
    "    columnKey (str): The key of the column whose unique values are to be printed.\n",
    "    sort_function (callable, optional): A function to sort the unique values. Defaults to None.\n",
    "    \"\"\"\n",
    "    unique_values = df[columnKey].unique().tolist()\n",
    "\n",
    "    # Apply sorting if a sort function is provided\n",
    "    if sort_function:\n",
    "        unique_values = sorted(unique_values, key=sort_function)\n",
    "\n",
    "    print(unique_values)\n",
    "\n",
    "\n",
    "# A custom function to fill the empty or nan values with a selected mod\n",
    "def num_filler(df, column=\"\", mode=\"median\"):\n",
    "    if df[column].dtype == \"object\":\n",
    "        print(f\"ERROR: The data-type of column {column} is not numerical!\")\n",
    "    else:\n",
    "        match mode:\n",
    "            case \"median\":\n",
    "                df_median = df[column].median()\n",
    "                df[column] = df[column].fillna(df_median)\n",
    "                return df[column]\n",
    "            case \"mean\":\n",
    "                df_mean = df[column].mean()\n",
    "                df[column] = df[column].fillna(df_mean)\n",
    "                return df[column]\n",
    "            case \"mod\":\n",
    "                df_mod = df[column].mod()\n",
    "                df[column] = df[column].fillna(df_mod)\n",
    "                return df[column]\n",
    "            case \"prob\":\n",
    "                value_counts = df[column].value_counts()\n",
    "                probabilities = value_counts / value_counts.sum()\n",
    "\n",
    "                # Randomly generate values for NaN based on the probabilities\n",
    "                random_values = np.random.choice(\n",
    "                    value_counts.index, size=df[column].isna().sum(), p=probabilities\n",
    "                )\n",
    "\n",
    "                # Fill the empty values with the randomly generated values\n",
    "                df.loc[df[column].isna(), column] = random_values\n",
    "                return df[column]\n",
    "            case _:\n",
    "                print(\n",
    "                    \"ERROR: mod is unvalid. Please choose one of the mods below:\\n median(default), mean, mod, prob\"\n",
    "                )\n",
    "                return df[column]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "\n",
    "def handle_incomplete_data(value, valid_values, default=\"Unknown\", cutoff=0.4):\n",
    "    if not isinstance(value, str):\n",
    "        return default\n",
    "\n",
    "    # Strip and attempt to find a close match\n",
    "    value = value.strip()\n",
    "    matches = get_close_matches(value, valid_values, n=1, cutoff=cutoff)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "\n",
    "def process_column(df, column, valid_values, default=\"Unknown\", cutoff=0.4):\n",
    "\n",
    "    return df[column].apply(\n",
    "        lambda x: handle_incomplete_data(x, valid_values, default, cutoff)\n",
    "    )\n",
    "\n",
    "\n",
    "def save_unique_values_to_file(df, column, filename):\n",
    "    unique_values = df[column].unique().tolist()\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "        return\n",
    "    with open(filename, \"w\") as file:\n",
    "        for value in unique_values:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "\n",
    "def removeSpecialChars(df, columnKey, chars=[]):\n",
    "    \"\"\"\n",
    "    Remove specified special characters from a column in the DataFrame. This function will be used for cleaning columns with basic strings with unwanted special characters; Not for replacing special characters with corresponding letters.\n",
    "    \"\"\"\n",
    "    for char in chars:\n",
    "        df[columnKey] = df[columnKey].str.replace(char, \"\", regex=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def removePartialStrings(df, columnKey, substrings=[]):\n",
    "    \"\"\"\n",
    "    Remove specified substrings from a column in the DataFrame. This function will be used for cleaning columns with basic strings with unwanted substrings.\n",
    "    \"\"\"\n",
    "    for substring in substrings:\n",
    "        df[columnKey] = df[columnKey].str.replace(substring, \"\", regex=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def findFuzzyGroups(df, column, threshold=0.85):\n",
    "    values = df[column].dropna().astype(str).unique().tolist()\n",
    "    visited = set()\n",
    "    groups = []\n",
    "\n",
    "    for i, v1 in enumerate(values):\n",
    "        if v1 in visited:\n",
    "            continue\n",
    "\n",
    "        group = [v1]\n",
    "        visited.add(v1)\n",
    "\n",
    "        for v2 in values[i + 1 :]:\n",
    "            if v2 in visited:\n",
    "                continue\n",
    "\n",
    "            similarity = SequenceMatcher(None, v1.lower(), v2.lower()).ratio()\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                group.append(v2)\n",
    "                visited.add(v2)\n",
    "\n",
    "        if len(group) > 1:\n",
    "            groups.append(group)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def capitalize_words(df, column):\n",
    "    df[column] = df[column].astype(str).str.lower().str.title()\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Import the raw data via pandas**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2026-01-16T16:51:07.545170800Z",
     "start_time": "2026-01-16T16:51:07.216566700Z"
    }
   },
   "source": [
    "#Import the raw dataset via pandas\n",
    "df= pd.read_csv('turkey_university_exam_score_dirty_v2.csv', sep=\",\") #sep=\";\" is needed since the seperator of the csv file is semicolon instead of comma"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'turkey_university_exam_score_dirty_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#Import the raw dataset via pandas\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m df= \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mturkey_university_exam_score_dirty_v2.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m,\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#sep=\";\" is needed since the seperator of the csv file is semicolon instead of comma\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject1\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'turkey_university_exam_score_dirty_v2.csv'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Display descriptive and technical info about the raw, dirty data**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df.describe().transpose().round(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MİN and MAX values are looking good"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "source": [
    "#display the head and tail\n",
    "display(df.head(), df.tail())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### *Column names should be fixed. Also, values should be unified on object type columns.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#display info about the df\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***Almost all of the columns should be renamed, according to the data description:***\n",
    "\n",
    "- *'student id'* to *'StudentID'*\n",
    "- *'EXAM_year'* to *'ExamYear'*\n",
    "- *' UniversityName'* to *'UniversityName'*  \n",
    "- *'Home-City'* to *'HomeCity'*\n",
    "- *'major'* to *'Major'*\n",
    "- *'GENDER'* to *'Gender'*\n",
    "- *'SocioEconomic_status'* to *'SocioEconomicStatus'*\n",
    "- *'PartTime job'* to *'PartTimeJob'*\n",
    "- *'StudyHours/Week'* to *'StudyHoursPerWeek'*  \n",
    "- *'Tutoring_Hours_per_Week'* to *'TutoringHoursPerWeek'*\n",
    "- *'attendance rate'* to *'AttendanceRate'*\n",
    "- *'priorGPA'* to *'PriorGPA'*\n",
    "\n",
    "***Also some data-types seem to be wrong:***\n",
    "- *'Enrollmentyear' should be converted from ```float64``` to ```int```*\n",
    "- *'ExamYear' should be converted from ```float64``` to ```int```*\n",
    "- *'SleepHours' should be converted from ```object``` to ```float64```*\n",
    "\n",
    "\n",
    "***Other columns look consistent with their dtypes and descriptions.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### *Check for duplicates and unique values (will check again later).*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#examine duplicates\n",
    "display(df.duplicated().sum(), df[df.duplicated(keep=False)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There are some duplicates. Lets investigate further.***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#check for duplicate rows without considering 'student id'\n",
    "dup_rows = df[df.duplicated(keep=False)]\n",
    "dup_rows.drop(columns=['student id']).duplicated().sum()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 duplicate pairs of rows that are identical in every column except student id. We will keep one of each pair and drop the other"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Keep one of each pair and drop the other.\n",
    "df = df.drop_duplicates(subset=df.columns.difference(['student id']), keep='first')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Check for the number of unique values\n",
    "print(df.nunique())\n",
    "print(\"\\n\")\n",
    "\n",
    "#save the unique values for each columns to specify the problems\n",
    "output_dir = \"unique_values\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # create dir if not exists\n",
    "\n",
    "\n",
    "#extract the unique values and save for each column\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    column_name = column.replace(\" \", \"_\").replace(\"/\", \"_\").lower()\n",
    "    file_path = os.path.join(output_dir, f\"{column_name}.txt\") # only will be used for specify the uniqe values of columns, so we dont need to save as csv,json etc.\n",
    "    #save unique values\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for value in unique_values:\n",
    "            file.write(f\"{value}\\n\")\n",
    "    \n",
    "    print(f\"Unique values for '{column}' saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# calculate the amount and percentage of NA/Null values on each column using our custom function, naValues\n",
    "naValues(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Although the missing data percentages are small we will not remove them directly.***\n",
    "\n",
    "***Data seems to be imported fine. Column headers are values. We don't have multiple variables in a column. Both rows and columns hold values. Every column has the same unit in itself.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Cleaning & Tidying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Make title names more appropriate**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2026-01-16T16:48:11.161220100Z",
     "start_time": "2026-01-16T16:48:11.008582100Z"
    }
   },
   "source": [
    "#column names from dataset description. ordered by dataset.csv columns\n",
    "column_names = [\n",
    "    \"Student_ID\",\n",
    "    \"Enrollment_Year\",\n",
    "    \"Exam_Year\",\n",
    "    \"University_Name\",\n",
    "    \"Home_City\",\n",
    "    \"Major\",\n",
    "    \"Gender\",\n",
    "    \"Scholarship_Status\",\n",
    "    \"Socioeconomic_Status\",\n",
    "    \"PartTime_Job\",\n",
    "    \"Study_Hours_per_Week\",\n",
    "    \"Tutoring_Hours_per_Week\",\n",
    "    \"Attendance_Rate\",\n",
    "    \"Sleep_Hours_per_Night\",\n",
    "    \"Prior_GPA\",\n",
    "    \"Exam_Score\"\n",
    "]\n",
    "#rename the column names by dataset description\n",
    "df.columns = column_names\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Convert columns to appropriate data types\n",
    "df['Sleep_Hours_per_Night'] = pd.to_numeric(df['Sleep_Hours_per_Night'], errors='coerce')\n",
    "\n",
    "df['Exam_Year'] = pd.to_numeric(df['Exam_Year'], errors='coerce').astype('Int64')\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m      2\u001B[39m column_names = [\n\u001B[32m      3\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mStudent_ID\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mEnrollment_Year\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     18\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mExam_Score\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     19\u001B[39m ]\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m#rename the column names by dataset description\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43mdf\u001B[49m.columns = column_names\n\u001B[32m     22\u001B[39m df.head()\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Convert columns to appropriate data types\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.describe().transpose().round(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enrollment_Year Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Enrollment_Year'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"Clean 'Enrollment_Year' column\"\"\"\n",
    "\n",
    "def cleanYearColumns(df, columnKey=\"Enrollment_Year\", startRange=2015, endRange=2021):\n",
    "    \"\"\"\n",
    "    Clean the 'Enrollment_Year' and 'Exam_Year' columns in one function (Since their cleaning process are identical):\n",
    "    - Ensure values are within the range 2015 to 2021.\n",
    "    - Handle missing values by filling them with the overall median year.\n",
    "    \"\"\"\n",
    "\n",
    "    df[columnKey] = pd.to_numeric(df[columnKey], errors='coerce').astype('Int64')\n",
    "    # Clip values to the valid range (2000 to 2024)\n",
    "    df[columnKey] = df[columnKey].clip(startRange, endRange)\n",
    "    \n",
    "    # Fill missing values with the overall median year\n",
    "    num_filler(df,columnKey,'median')\n",
    "    \n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "cleanYearColumns(df,\"Enrollment_Year\",2015,2021)\n",
    "cleanYearColumns(df,\"Exam_Year\",2015,2021)\n",
    "\"\"\"\n",
    "- print unique values of column\n",
    "- since our years are not a big data, we can manually validate our cleaned column data\n",
    "\"\"\"\n",
    "printUniqueValues(df,\"Enrollment_Year\",sort_function=lambda x: x)\n",
    "printUniqueValues(df,\"Exam_Year\",sort_function=lambda x: x)\n",
    "#saveFile(\"processing_dataset.csv\",df_years_cleaned.to_csv(index=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **University_Name Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df[\"University_Name\"].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It can be clearly seen that there are same university names with letter capitalization problems or used short names of universities. There is also some special characters that is not used for any letter replacement***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "str_basicclean(df,'University_Name',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "print(len(df['University_Name'].unique()))\n",
    "#Now we have 27 unique university names after lowercasing and stripping whitespaces, lets save them to a file for visual inspection\n",
    "save_unique_values_to_file(df, 'University_Name', 'processing/university_name.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visual inspection, we decided to remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Remove special characters '!' and '#' from 'University_Name' column\n",
    "removeSpecialChars(df, 'University_Name', chars=['!','#'])\n",
    "save_unique_values_to_file(df, 'University_Name', 'processing/university_name.txt')\n",
    "print(len(df['University_Name'].unique()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After previous cleaning steps, we are succesfully reduced  68 unique values to 17. Now we visually identify that there are some typos \"university\" or \"university\" so lets remove that suffixes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# remove 'university' or 'universty' suffixes from names\n",
    "removePartialStrings(df, 'University_Name', substrings=[' university', ' universty'])\n",
    "\n",
    "save_unique_values_to_file(df, 'University_Name', 'processing/university_name.txt')\n",
    "print(len(df['University_Name'].unique()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually map the remained 13 unique values and fill the NaN rows with \"Other Universities\" "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#manually created dictionary of correct university names based on visual inspection of unique values\n",
    "UniversityNames = {\n",
    "    \"yildiz technical\": \"Yildiz Technical University\",\n",
    "    'bilkent': \"Bilkent University\",\n",
    "    'hacettepe': \"Hacettepe University\",\n",
    "    'istanbul': \"Istanbul University\",\n",
    "    'koc': \"Koc University\",\n",
    "    'bogazici': \"Bogazici University\",\n",
    "    'ankara': \"Ankara University\",\n",
    "    'ege': \"Ege University\",\n",
    "    'gazi': \"Gazi University\",\n",
    "    'metu': \"Middle East Technical University\",\n",
    "    'dokuz eylul': \"Dokuz Eylul University\",\n",
    "    'itu': \"Istanbul Technical University\",\n",
    "}\n",
    "#process the 'University_Name' column using the manually created dictionary\n",
    "df['University_Name'] = process_column(df, 'University_Name', list(UniversityNames.keys()), default='Other Universities', cutoff=0.4).map(UniversityNames).fillna('Other Universities')\n",
    "save_unique_values_to_file(df, 'University_Name', 'processing/university_name.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['University_Name'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University_Name column cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Home_City Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Home_City'].describe())\n",
    "display(df['Home_City'].unique())\n",
    "save_unique_values_to_file(df, 'Home_City', 'processing/home_city.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Home_City column also has the same problems with University_Name column. So the same steps will be follwed.***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "str_basicclean(df,'Home_City',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "print(len(df['Home_City'].unique()))\n",
    "#Now we have 27 unique university names after lowercasing and stripping whitespaces, lets save them to a file for visual inspection\n",
    "save_unique_values_to_file(df, 'Home_City', 'processing/home_city.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Remove special characters '!' and '#' from 'Home_City' column\n",
    "removeSpecialChars(df, 'Home_City', chars=['!','#'])\n",
    "save_unique_values_to_file(df, 'Home_City', 'processing/home_city.txt')\n",
    "print(len(df['Home_City'].unique()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "similar_cities = findFuzzyGroups(df, 'Home_City', threshold=0.7)\n",
    "print(f\"Similar cities to {similar_cities}\")\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have much typos so lets map them and fill the NaN values with \"Other Cities\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We don't have much typos so lets map them and fill the NaN values with \"Other Cities\" using similar_cities varaible:  [['ankara', 'ankraa'], ['izmir', 'i̇zmir'], ['istanbul', 'istambul'], ['diyarbakir', 'diyarbakır']]\n",
    "\n",
    "df[\"Home_City\"] = (\n",
    "    df[\"Home_City\"]\n",
    "    .replace(\n",
    "        {\n",
    "            \"ankraa\": \"ankara\",\n",
    "            \"i̇zmir\": \"izmir\",\n",
    "            \"istambul\": \"istanbul\",\n",
    "            \"diyarbakır\": \"diyarbakir\",\n",
    "        }\n",
    "    )\n",
    "    .fillna(\"Other Cities\")\n",
    ")\n",
    "df[\"Home_City\"] = str_basicclean(df, \"Home_City\", style=\"cap\")\n",
    "\n",
    "save_unique_values_to_file(df, \"Home_City\", \"processing/home_city.txt\")\n",
    "df[\"Home_City\"].describe()\n",
    "df['Home_City'].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***After visually inspecting the Home_City column, we can conclude its cleaned***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Major Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Major'].describe())\n",
    "display(df['Major'].unique())\n",
    "save_unique_values_to_file(df, 'Major', 'processing/major.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Major column also has the same problems with University_Name and Home_City column. So the similar steps will be followed.***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "str_basicclean(df,'Major',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "\n",
    "removeSpecialChars(df, 'Major', chars=['!','#'])\n",
    "print(len(df['Major'].unique()))\n",
    "save_unique_values_to_file(df, 'Major', 'processing/major.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fuzzyMatches = findFuzzyGroups(df, 'Major', threshold=0.8)\n",
    "print(f\"Similar majors to {fuzzyMatches}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['Major'] = df['Major'].replace({\n",
    "    'comp. science': 'computer science',\n",
    "    'enginering': 'engineering',\n",
    "    'econnomics': 'economics',\n",
    "}).fillna('Other Majors')\n",
    "\n",
    "capitalize_words(df, \"Major\")\n",
    "save_unique_values_to_file(df, 'Major', 'processing/major.txt')\n",
    "df['Major'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Major column is clean now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gender Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Gender'].describe())\n",
    "display(df['Gender'].unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The Gender column has minor typos, capitalziation issues or special characters in strings. Its easy to clean this column .***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "str_basicclean(df,'Gender',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "\n",
    "removeSpecialChars(df, 'Gender', chars=['!','#'])\n",
    "df['Gender'] = df['Gender'].fillna('Unknown').astype(str)\n",
    "capitalize_words(df, \"Gender\")\n",
    "save_unique_values_to_file(df, 'Gender', 'processing/gender.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['Gender'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick cleaning steps to clean Gender column. Now the Gender column is clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Scholarship_Status Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Scholarship_Status'].describe())\n",
    "display(df['Scholarship_Status'].unique())\n",
    "save_unique_values_to_file(df, 'Scholarship_Status', 'processing/scholarship_status.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scholarship_Status column has same problems as Gender column. Sme steps will be applied directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "str_basicclean(df,'Scholarship_Status',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "\n",
    "removeSpecialChars(df, 'Scholarship_Status', chars=['!','#'])\n",
    "df['Scholarship_Status'] = df['Scholarship_Status'].fillna('Unknown').astype(str)\n",
    "capitalize_words(df, \"Scholarship_Status\")\n",
    "save_unique_values_to_file(df, 'Scholarship_Status', 'processing/scholarship_status.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scholarship_Status colum ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Socioeconomic_Status Column**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Socioeconomic_Status'].describe())\n",
    "display(df['Socioeconomic_Status'].unique())\n",
    "save_unique_values_to_file(df, 'Socioeconomic_Status', 'processing/socioeconomic_status.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socioeconomic_Status column has exact same problems with Gender and Scholarship_Status columns. Exact steps will be applied."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "str_basicclean(df,'Socioeconomic_Status',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "\n",
    "removeSpecialChars(df, 'Socioeconomic_Status', chars=['!','#'])\n",
    "capitalize_words(df, \"Socioeconomic_Status\")\n",
    "save_unique_values_to_file(df, 'Socioeconomic_Status', 'processing/socioeconomic_status.txt')\n",
    "df['Socioeconomic_Status'].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make Socioeconomic_Status column  - categorical, ordinal-  type, fill the NaN values with median"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Lets map the socioeconomic status to numerical values for easier median calculation and filling the NaN values\n",
    "socioeconomic_mapping = {\n",
    "    'Low': 1,\n",
    "    'Middle': 2,\n",
    "    'High': 3,\n",
    "    'Nan': 0\n",
    "}\n",
    "#map the values\n",
    "df['ses_num'] = df['Socioeconomic_Status'].map(socioeconomic_mapping)\n",
    "#calculate median excluding 0 (which represents NaN)\n",
    "median_ses = df['ses_num'].median()\n",
    "#replace 0 and NaN with median\n",
    "df['ses_num'] = df['ses_num'].replace(0, median_ses)\n",
    "df['ses_num'] = df['ses_num'].fillna(median_ses)\n",
    "#map back to original string values\n",
    "reverse_map = {1:'Low', 2:'Middle', 3:'High'}\n",
    "#finalize the 'Socioeconomic_Status' column\n",
    "df['Socioeconomic_Status'] = df['ses_num'].round().map(reverse_map)\n",
    "#drop the temporary numerical column\n",
    "df = df.drop(columns=['ses_num'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['Socioeconomic_Status'].describe()\n",
    "df['Socioeconomic_Status'].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set 'Socioeconomic_Status' as an ordered categorical variable dtype\n",
    "df['Socioeconomic_Status'] = pd.Categorical(\n",
    "    df['Socioeconomic_Status'],\n",
    "    categories=['Low', 'Middle', 'High'],\n",
    "    ordered=True\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Socioeconomic_Status column cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**PartTime_Job Column**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df[\"PartTime_Job\"].describe()\n",
    "df[\"PartTime_Job\"].unique()\n",
    "save_unique_values_to_file(df, 'PartTime_Job', 'processing/parttime_job.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PartTime_Job colum has same problems as Socioeconomic_Status or Scholarship_Status so same steps will be applied."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "source": [
    "str_basicclean(df,'PartTime_Job',style=\"low\") #Uses custom function str_basicclean to strip the characters, and deal with the problem with some of\n",
    "\n",
    "removeSpecialChars(df, 'PartTime_Job', chars=['!','#'])\n",
    "capitalize_words(df, \"PartTime_Job\")\n",
    "df['PartTime_Job'] = df['PartTime_Job'].fillna('Unknown').replace('Nan','Unknown').astype(str)\n",
    "save_unique_values_to_file(df, 'PartTime_Job', 'processing/parttime_job.txt')\n",
    "df['PartTime_Job'].unique()\n",
    "df['PartTime_Job'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the PartTime_Job column is clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Study_Hours_per_Week Column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['Study_Hours_per_Week'].describe()\n",
    "df['Study_Hours_per_Week'].unique()\n",
    "save_unique_values_to_file(df, 'Study_Hours_per_Week', 'processing/study_hours_per_week.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Study_Hours_per_Week column is continous ratio column. So It must be between 0-168 hours. And we need to fill the NaNs with median"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#The Study_Hours_per_Week column is continous ratio column. So It must be between 0-168 hours. And we need to fill the NaNs with median\n",
    "df['Study_Hours_per_Week'] = pd.to_numeric(df['Study_Hours_per_Week'], errors='coerce')\n",
    "df['Study_Hours_per_Week'] = df['Study_Hours_per_Week'].clip(0, 168)\n",
    "num_filler(df, 'Study_Hours_per_Week', mode='median')\n",
    "df['Study_Hours_per_Week'].describe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've cleaned te \"Study_Hours_per_Week\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tutoring_Hours_per_Week Column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Tutoring_Hours_per_Week'].unique())\n",
    "display(df['Tutoring_Hours_per_Week'].describe())\n",
    "save_unique_values_to_file(df, 'Tutoring_Hours_per_Week', 'processing/tutoring_hours_per_week.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only filling the NaN/NA values with median will be enough for this column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_filler(df, 'Tutoring_Hours_per_Week', mode='median')\n",
    "df['Tutoring_Hours_per_Week'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attendance_Rate Column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Attendance_Rate'].unique())\n",
    "display(df['Attendance_Rate'].describe())\n",
    "save_unique_values_to_file(df, 'Attendance_Rate', 'processing/attendance_rate.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only filling missing values will be enough for \"Attendance_Rate\" column as well"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_filler(df, 'Attendance_Rate', mode='median')\n",
    "df['Attendance_Rate'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sleep_Hours_per_Night Column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Sleep_Hours_per_Night'].unique())\n",
    "display(df['Sleep_Hours_per_Night'].describe())\n",
    "save_unique_values_to_file(df, 'Sleep_Hours_per_Night', 'processing/sleep_hours_per_night.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_filler(df, 'Sleep_Hours_per_Night', mode='median')\n",
    "df['Sleep_Hours_per_Night'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prior_GPA Column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Prior_GPA'].unique())\n",
    "display(df['Prior_GPA'].describe())\n",
    "save_unique_values_to_file(df, 'Prior_GPA', 'processing/prior_gpa.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_filler(df, 'Prior_GPA', mode='median')\n",
    "df['Prior_GPA'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Exam_Score Column"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(df['Exam_Score'].unique())\n",
    "display(df['Exam_Score'].describe())\n",
    "save_unique_values_to_file(df, 'Exam_Score', 'processing/exam_score.txt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_filler(df, 'Exam_Score', mode='median')\n",
    "df['Exam_Score'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Export the Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "print(\"Cleaned data has been exported to 'cleaned_data.csv'.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import the Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df= pd.read_csv('cleaned_data.csv')\n",
    "df_copy = df.copy()\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df.describe().transpose().round(3)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
